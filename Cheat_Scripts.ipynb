{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    # let's only gather Time U.S. magazine covers\n",
    "    url = response.css(\"div.refineCol ul li\").xpath(\"a[contains(., 'TIME U.S.')]\")\n",
    "    yield scrapy.Request(url.xpath(\"@href\").extract_first(), self.parse_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some urls with certain callback and other urls with a different callback:\n",
    "\n",
    "from scrapy.spiders import SitemapSpider\n",
    "\n",
    "class MySpider(SitemapSpider):\n",
    "    sitemap_urls = ['http://www.example.com/sitemap.xml']\n",
    "    sitemap_rules = [\n",
    "        ('/product/', 'parse_product'),\n",
    "        ('/category/', 'parse_category'),\n",
    "    ]\n",
    "\n",
    "    def parse_product(self, response):\n",
    "        pass # ... scrape product ...\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        pass # ... scrape category ...\n",
    "    \n",
    "    \n",
    "# Combine SitemapSpider with other sources of urls:\n",
    "from scrapy.spiders import SitemapSpider\n",
    "\n",
    "class MySpider(SitemapSpider):\n",
    "    sitemap_urls = ['http://www.example.com/robots.txt']\n",
    "    sitemap_rules = [\n",
    "        ('/shop/', 'parse_shop'),\n",
    "    ]\n",
    "\n",
    "    other_urls = ['http://www.example.com/about']\n",
    "\n",
    "    def start_requests(self):\n",
    "        requests = list(super(MySpider, self).start_requests())\n",
    "        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]\n",
    "        return requests\n",
    "\n",
    "    def parse_shop(self, response):\n",
    "        pass # ... scrape shop here ...\n",
    "\n",
    "    def parse_other(self, response):\n",
    "        pass # ... scrape other here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hereâ€™s an example spider logging all errors and catching some specific errors if needed:\n",
    "\n",
    "\n",
    "import scrapy\n",
    "\n",
    "from scrapy.spidermiddlewares.httperror import HttpError\n",
    "from twisted.internet.error import DNSLookupError\n",
    "from twisted.internet.error import TimeoutError, TCPTimedOutError\n",
    "\n",
    "class ErrbackSpider(scrapy.Spider):\n",
    "    name = \"errback_example\"\n",
    "    start_urls = [\n",
    "        \"http://www.httpbin.org/\",              # HTTP 200 expected\n",
    "        \"http://www.httpbin.org/status/404\",    # Not found error\n",
    "        \"http://www.httpbin.org/status/500\",    # server issue\n",
    "        \"http://www.httpbin.org:12345/\",        # non-responding host, timeout expected\n",
    "        \"http://www.httphttpbinbin.org/\",       # DNS error expected\n",
    "    ]\n",
    "\n",
    "    def start_requests(self):\n",
    "        for u in self.start_urls:\n",
    "            yield scrapy.Request(u, callback=self.parse_httpbin,\n",
    "                                    errback=self.errback_httpbin,\n",
    "                                    dont_filter=True)\n",
    "\n",
    "    def parse_httpbin(self, response):\n",
    "        self.logger.info('Got successful response from {}'.format(response.url))\n",
    "        # do something useful here...\n",
    "\n",
    "    def errback_httpbin(self, failure):\n",
    "        # log all failures\n",
    "        self.logger.error(repr(failure))\n",
    "\n",
    "        # in case you want to do something special for some errors,\n",
    "        # you may need the failure's type:\n",
    "\n",
    "        if failure.check(HttpError):\n",
    "            # these exceptions come from HttpError spider middleware\n",
    "            # you can get the non-200 response\n",
    "            response = failure.value.response\n",
    "            self.logger.error('HttpError on %s', response.url)\n",
    "\n",
    "        elif failure.check(DNSLookupError):\n",
    "            # this is the original request\n",
    "            request = failure.request\n",
    "            self.logger.error('DNSLookupError on %s', request.url)\n",
    "\n",
    "        elif failure.check(TimeoutError, TCPTimedOutError):\n",
    "            request = failure.request\n",
    "            self.logger.error('TimeoutError on %s', request.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login through form in scrapy\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class LoginSpider(scrapy.Spider):\n",
    "    name = 'example.com'\n",
    "    start_urls = ['http://www.example.com/users/login.php']\n",
    "\n",
    "    def parse(self, response):\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'username': 'john', 'password': 'secret'},\n",
    "            callback=self.after_login\n",
    "        )\n",
    "\n",
    "    def after_login(self, response):\n",
    "        # check login succeed before going on\n",
    "        if \"authentication failed\" in response.body:\n",
    "            self.logger.error(\"Login failed\")\n",
    "            return\n",
    "\n",
    "        # continue scraping with authenticated session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link extractors are objects whose only purpose is to extract links from web pages\n",
    "# (scrapy.http.Response objects) which will be eventually followed.\n",
    "\n",
    "from scrapy.linkextractors import LinkExtractor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
